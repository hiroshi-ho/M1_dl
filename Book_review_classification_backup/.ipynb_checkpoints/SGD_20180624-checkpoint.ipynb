{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13692:1 3133:1 111986:1 238:1 18955:1 111987:1 111988:1 111989:1 361:1 111990:1 27:1 111991:1 111992:2 111993:1 12280:1 111994:1 111995:1 111996:1 2752:1 26836:1 111997:1 92667:1 111998:1 66257:1 91802:1 6083:1 80407:1 721:1 111999:1 34216:1 2761:1 112000:1 112001:1 112002:1 12:1 79197:1 19070:1 22737:1 112003:1 2478:1 5808:1 33330:1 112004:1 112005:1 112006:1 3658:1 112007:1 100900:1 112008:1 112009:1 399:1 112010:1 5142:1 3523:1 112011:2 22142:1 112012:1 112013:1 81038:1 1154:2 9126:1 2699:1 112014:1 112015:1 11177:1 112016:1 112017:1 3203:1 31155:1 51621:2 520:1 25367:1 14840:1 1419:1 2389:1 112018:1 112019:1 12690:1 112020:1 43:1 10414:1 363:1 67922:1 28326:1 112021:1 99099:1 5171:1 11250:3 1840:1 7560:1 11947:1 62:1 81986:3 6481:1 112022:1 767:1 3497:1 74319:2 112023:1 79077:1 6489:1 112024:3 112025:1 785:1 112026:1 14111:1 66125:1 112027:1 112028:1 17995:1 72578:1 112029:1 112030:1 112031:1 2255:1 25368:1 13368:1 112032:1 112033:1 112034:1 28347:1 112035:1 1784:1 112036:1 112037:1 3012:1 1041:1 1534:1 406:2 1127:1 5517:2 2882:1 112038:1 31333:2 112039:1 1643:1 1449:1 112040:1 112041:1 83526:1 112042:1 75962:1 112043:1 91736:1 358:1 564:1 109421:1 112044:1 1880:1 9085:1 112045:1 63517:1 112046:1 112047:1 112048:1 2840:2 7399:4 112049:1 112050:1 112051:1 57711:1 56:1 761:3 4744:1 112052:1 73625:1 766:1 5030:1 112053:1 112054:2 66:1 112055:1 112056:1 45942:1 112057:1 143:4 112058:2 112059:1 112060:1 112061:1 11494:1 26894:1 32621:1 112062:1 41:1 112063:1 1118:1 112064:1 112065:1 754:1 1038:2 112066:1 38881:1 42785:1 112067:1 55428:1 1638:1 1325:1 78561:1 17152:1 1739:1 112068:1 87766:1 112069:1 112070:1 80304:2 112071:1 589:1 92739:1 1149:1 3382:3 112072:1 112073:1 3093:1 112074:1 16640:1 5753:1 112075:1 15895:1 4438:1 33438:1 23851:3 112076:1 10604:1 112077:1 7192:1 112078:1 7121:1 108:1 21482:1 1532:1 112079:1 7108:1 24529:1 112080:1 99090:1 112081:1 112082:1 62378:1 7132:1 112083:1 112084:1 112085:1 379:1 112086:1 54792:1 21321:1 112087:1 33:2 112088:1 112089:1 68279:4 112090:1 112091:1 112092:1 52987:1 112093:1 112094:1 8324:1 103767:1 30659:1 5712:1 53042:1 746:2 24409:1 112095:2 92769:1 107:1 7313:1 5616:1\n",
      "\n",
      "4011:1 112096:1 2582:1 62596:2 112097:2 5296:1 90166:1 112098:1 15813:1 1364:1 112099:1 112100:1 66:2 4394:1 9468:1 12:2 112101:1 4304:1 71548:1 832:1 90110:1 112102:1 112103:1 33857:1 112104:1 112105:1 7055:1 112106:2 5837:1 1255:1 93382:1 112107:1 29877:1 2723:1 112108:1 27:3 2179:1 112109:1 112110:1 112111:1 45641:1 112112:1 8751:2 112113:1 12646:1 8491:2 112114:1 41458:1 112115:1 2031:1 112116:1 112117:1 60711:1 12648:1 2662:1 112118:1 13711:1 112119:1 1583:1 34839:1 112120:1 112121:1 5821:1 2667:1 93482:1 1545:1 25706:1 341:1 112122:1 586:1 7066:1 112123:1 112124:1 43725:1 56:4 112125:1 8004:1 112126:1 112127:1 20678:1 5636:1 42473:1 1069:1 112128:1 4867:1 4117:1 90:1 112129:1 826:1 27217:1 10783:1 112130:2 440:1 992:2 2686:1 5171:1 10383:1 6704:1 112131:1 90005:1 112132:2 112133:1 112134:1 112135:1 1608:1 61010:1 238:2 79609:1 112136:1 112137:1 112138:1 871:2 53750:1 112139:1 112140:1 112141:1 112142:1 112143:1 19602:1 112144:1 11371:1 112145:1 12517:1 112146:1 112147:1 79588:1 112148:1 108:4 769:1 42:1 112149:1 15848:1 1237:1 60954:1 51406:1 877:1 112150:1 112151:3 90953:1 23201:1 5558:1 5945:1 7873:1 112152:1 2296:1 1045:1 1325:1 227:1 112153:3 7709:1 112154:1 9098:1 10658:8 112155:1 6678:3 112156:1 31990:1 10806:1 14939:1 5211:1 8473:1 62519:1 7607:1 140:1 112157:2 17284:1 285:2 27123:1 2236:2 112158:1 17906:1 112159:1 14736:1\n",
      "\n",
      "112160:1 112161:1 8359:1 112162:1 4636:1 12:1 112163:1 6513:1 112164:1 16461:1 112165:1 112166:1 112167:1 6938:1 50236:1 112168:1 112169:1 12646:1 112170:1 59255:2 11510:1 1201:1 26795:1 26785:1 112171:1 12236:1 1050:1 8:1 617:2 75022:1 42276:1 108:2 11307:1 112172:1 844:2 5931:1 112173:1 90:1 112174:1 112175:1 2179:1 112176:1 1525:1 23743:1 112177:1 112178:1 26945:1 112179:1 112180:1 26990:1 112181:1 112182:1 417:1 112183:1 112184:1 258:1 112185:1 112186:1 7223:1 112187:1 112188:1 12340:1 56:1 112189:1 112190:1 112191:1 112192:1 112193:1 112194:1 4813:1 112195:1 12443:2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "#データの中身を見る\n",
    "with open(\"./data/books/positive.review\",mode=\"r\",encoding = \"utf-8\") as p:\n",
    "    counter = 0\n",
    "    for line in p:\n",
    "        print(line)\n",
    "        counter +=1\n",
    "        if counter ==3:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 sentence is vectorised\n",
      "500 sentence is vectorised\n",
      "0 sentence is vectorised\n",
      "500 sentence is vectorised\n",
      "2000 sentence is vectorised\n"
     ]
    }
   ],
   "source": [
    "#データをベクトル化する\n",
    "# -*- coding: utf-8 -*-\n",
    "import math\n",
    "import csv\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "def sentence_vectorize():\n",
    "    \n",
    "    #positive,negativeすべて合わせた文のインデックス用\n",
    "    #後でシャッフル呼び出し時に用いる\n",
    "    total_index = 0\n",
    "    \n",
    "    #positive,negative両方用\n",
    "    all_sentence_vectorized = []\n",
    "    #positive : sign =1, negative : sign = -1\n",
    "    with open(\"./data/books/positive.review\",mode = \"r\", encoding = \"utf-8\") as p:\n",
    "        index = 0   #各文にインデックスを付ける。シャッフル呼び出し用\n",
    "        positive = []\n",
    "        for line in p:\n",
    "            \n",
    "            oneline = line.split(\" \") #まだ　\" \" でsplitしただけ\n",
    "            one_sentence = [0 for _ in range(200000)]  #これから一文をベクトル化する\n",
    "            \n",
    "            #一文内の各id:countに対する処理\n",
    "            for one_id_count_set in oneline:\n",
    "                id_,count_ = one_id_count_set.split(\":\")\n",
    "                # 文末のid:count用\n",
    "                count_ = count_.strip() \n",
    "                \n",
    "                one_sentence[int(id_)] += int(count_)/50\n",
    "                # / 10000 はあとのシグモイド中の爆発を抑えるため\n",
    "            #一文に対する処理　ここまで\n",
    "            #one_sentenceを記録する、ただし、sign,indexをつける必要がある\n",
    "            \n",
    "            #sign, index用も込のリスト、すなわち\n",
    "            #[ sign ,  index,  [ 200000次元のリスト ]]　とする。\n",
    "            \n",
    "            \n",
    "            \n",
    "            if index % 500 == 0:\n",
    "                print(str(index) + \" sentence is vectorised\")\n",
    "            \n",
    "            positive.append(one_sentence)\n",
    "                \n",
    "            #各sentenceに固有\n",
    "            index +=1\n",
    "        \n",
    "        with open(\"./positive.review.vector\",\"wb\") as pp:\n",
    "            pickle.dump(positive,pp)\n",
    "        \n",
    "        #negative_dataの各文章に対するインデックス開始は1から\n",
    "        #total_indexは全センテンス用\n",
    "        total_index += index\n",
    "        all_sentence_vectorized.append(positive)\n",
    "    #繰り返しは良くないが、うまい方法を思いつかなかった、以下negative.review用\n",
    "    with open(\"./data/books/negative.review\",mode = \"r\", encoding = \"utf-8\") as n:\n",
    "        index = 0   #各文にインデックスを付ける。シャッフル呼び出し用\n",
    "        negative = []\n",
    "        for line in n:\n",
    "            \n",
    "            oneline = line.split(\" \") #まだ　\" \" でsplitしただけ\n",
    "            one_sentence = [0 for _ in range(200000)]  #これから一文をベクトル化する\n",
    "            \n",
    "            #一文内の各id:countに対する処理\n",
    "            for one_id_count_set in oneline:\n",
    "                id_,count_ = one_id_count_set.split(\":\")\n",
    "                # 文末のid:count用\n",
    "                count_ = count_.strip() \n",
    "                \n",
    "                one_sentence[int(id_)] += int(count_)/50\n",
    "            #一文に対する処理　ここまで\n",
    "            #one_sentenceを記録する、ただし、sign,indexをつける必要がある\n",
    "            \n",
    "            #sign, index用も込のリスト、すなわち\n",
    "            #[ sign ,  index,  [ 200000次元のリスト ]]　とする。\n",
    "            \n",
    "            sentence_to_be_recorded = [index,-1,one_sentence]\n",
    "            \n",
    "            if index % 500 == 0:\n",
    "                print(str(index) + \" sentence is vectorised\")\n",
    "            \n",
    "            negative.append(one_sentence)\n",
    "                \n",
    "            #各sentenceに固有\n",
    "            index +=1\n",
    "            total_index +=1\n",
    "    all_sentence_vectorized.append(negative)\n",
    "    with open(\"./negative.review.vector\",\"wb\") as nn:\n",
    "            pickle.dump(negative,nn) \n",
    "            \n",
    "        \n",
    "    with open(\"./all_sentence_vectorised.vector\",\"wb\") as f:\n",
    "        pickle.dump(all_sentence_vectorized,f)\n",
    "        \n",
    "    print(str(total_index) + \" sentence is vectorised\")\n",
    "    \n",
    "sentence_vectorize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport pickle\\nwith open(\"all_sentence_vectorised.vector\",\"rb\") as f:\\n    alllist = pickle.load(f)     \\n    for i in range(1000):\\n        print(alllist[-1][1][i]) \\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#データの確認\n",
    "\n",
    "\"\"\"\n",
    "pickle =   [ \n",
    "                   [ [positive１文目],[positive2文目], ...[positive1000文目] ],\n",
    "                   [ [negative１文目],[negative2文目], ...[negative1000文目] ]\n",
    "                   \n",
    "                ]\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "import pickle\n",
    "with open(\"all_sentence_vectorised.vector\",\"rb\") as f:\n",
    "    alllist = pickle.load(f)     \n",
    "    for i in range(1000):\n",
    "        print(alllist[-1][1][i]) \n",
    "\"\"\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split train=0.8, dev = 0.1, test = 0.1\n",
    "\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "def test_split():\n",
    "    #for positive split\n",
    "    #shuffle raw positive data →split\n",
    "    positive_train = []\n",
    "    positive_dev = []\n",
    "    positive_test = []\n",
    "    \n",
    "    negative_train = []\n",
    "    negative_dev = []\n",
    "    negative_test = []\n",
    "    \n",
    "    with open(\"positive.review.vector\",\"wb\") as p:\n",
    "        positive_list = pickle.load(p)\n",
    "        random.shuffle(positive_list)\n",
    "        for i in range(len(1000)):\n",
    "            if i <800:\n",
    "                positive_train = \n",
    "                \n",
    "        pickle.dump(positive_list,p) \n",
    "        \n",
    "    with open(\"negative.review.vector\",\"wb\") as n:\n",
    "        negagtive_list = pickle.load(n)\n",
    "        random.shuffle(negative_list)\n",
    "        pickle.dump(negative_list,n) \n",
    "        \n",
    "    with open(\"./positive_dev_list\",\"wb\") as f:\n",
    "        pickle.dump(positive_test_list,f) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#関数の定義\n",
    "#http://ai-lab.info/wp-content/uploads/2017/11/%EF%BC%B0%EF%BC%B9%EF%BC%B4%EF%BC%A8%EF%BC%AF%EF%BC%AE%E3%81%A7%E5%AD%A6%E3%81%B6%E6%A9%9F%E4%BC%9A%E5%AD%A6%E7%BF%92.pdf\n",
    "\n",
    "import math\n",
    "def sigmoid(x):\n",
    "    return (1 / (1 + math.exp(-x)))\n",
    "\n",
    "def inner_product(list_x,list_y):\n",
    "    sums = 0\n",
    "    for i in range(len(list_x)):\n",
    "        sums+= list_x[i] * list_y[i]\n",
    "    return sums\n",
    "\n",
    "def plus(list_x,list_y):\n",
    "    result = [0 for _ in range(len(list_x))]\n",
    "    for i in range(len(list_x)):\n",
    "        result[i] = list_x[i] + list_y[i]\n",
    "    return result    \n",
    "\n",
    "def cross(a,list_x):\n",
    "    for i in list_x:\n",
    "        i = a * i\n",
    "    return list_x\n",
    "\n",
    "def onesentence_accuracy_calc(w_params_fixed,sentence,y_answer): #positive: y=1, negative: y = -1\n",
    "    s = inner_product(w_params_fixed,sentence)\n",
    "    if (y_answer ==1and  s >= 0) or (y_answer == -1 and s <0):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "        \n",
    "\"\"\"\n",
    "for n batch(n = 30 ~ 100):\n",
    "     w ← w - η * (1 / n) * Σ_(n batch　のsum,ただしwは更新前に固定)_  (-y*x) / (1 + exp(y * <w,x>))\n",
    "     #　η: learning_rate\n",
    "\"\"\"\n",
    "\n",
    "def minibatch_Loss_and_accuracy_calc_for_test(w_params,batch_size):\n",
    "    Loss_and_accuracy = []\n",
    "    with open(\"./positive_test_list\",\"rb\") as p:\n",
    "        with open(\"./negative_test_list\",\"rb\") as n:\n",
    "            \n",
    "            p_test_list = pickle.load(p)\n",
    "            n_test_list = pickle.load(n)\n",
    "            \n",
    "            sentence_index = [i for i in range(0,200)]\n",
    "            positive_test_index = random.sample(sentence_index,batch_size)\n",
    "            negative_test_index = random.sample(sentence_index,batch_size)\n",
    "            \n",
    "            Loss_for_minibatch = 0\n",
    "            accuracy_count = 0\n",
    "            for i in positive_test_index:\n",
    "                Loss_for_minibatch += (-1) * math.log(sigmoid(1 * inner_product(w_params,p_test_list[i])))\n",
    "                #print(sigmoid(1*inner_product(w_params,p_test_list[i])))\n",
    "                accuracy_count += onesentence_accuracy_calc(w_params,p_test_list[i],y_answer = 1)\n",
    "            for j in negative_test_index:\n",
    "                Loss_for_minibatch += (-1) * math.log(sigmoid((-1) * inner_product(w_params,n_test_list[j])))\n",
    "                #print(sigmoid(-1*inner_product(w_params,n_test_list[j])))\n",
    "                accuracy_count += onesentence_accuracy_calc(w_params,n_test_list[j],y_answer = -1)\n",
    "            Loss_for_minibatch = Loss_for_minibatch / (batch_size * 2)\n",
    "            accuracy = accuracy_count / (batch_size * 2)\n",
    "            Loss_and_accuracy.append(Loss_for_minibatch)\n",
    "            Loss_and_accuracy.append(accuracy)\n",
    "    return  Loss_and_accuracy\n",
    "\n",
    "def training(w_params,batch_size,learning_rate):\n",
    "    w_params_before_changed = w_params\n",
    "    with open(\"./positive_train_list\",\"rb\") as p:\n",
    "        with open(\"./negative_train_list\",\"rb\") as n:    \n",
    "            \n",
    "            p_train_list = pickle.load(p)\n",
    "            n_train_list = pickle.load(n)\n",
    "            \n",
    "            sentence_index = [i for i in range(0,800)]\n",
    "            positive_train_index = random.sample(sentence_index,batch_size)\n",
    "            negative_train_index = random.sample(sentence_index,batch_size)\n",
    "            \n",
    "            # ∂E/ ∂w  = (- y* x)σ(-y * (<w ,x> ) )\n",
    "            gradE_sums_for_minibatch = [0 for _ in range(200000)]  \n",
    "            for i in positive_train_index:\n",
    "                gradE_sums_for_minibatch = plus(gradE_sums_for_minibatch, cross( ((-1) * sigmoid((-1) * inner_product(w_params,p_train_list[i])) ), p_train_list[i]) )\n",
    "            for j in negative_train_index:\n",
    "                gradE_sums_for_minibatch = plus(gradE_sums_for_minibatch, cross( ((1) * sigmoid((1) * inner_product(w_params,n_train_list[j])) ), n_train_list[j]) )\n",
    "            \n",
    "            Loss = 0\n",
    "            for i in positive_train_index:\n",
    "                Loss += (-1) * math.log(sigmoid(1 * inner_product(w_params,p_train_list[i])))\n",
    "            for j in negative_train_index:\n",
    "                Loss += (-1) * math.log(sigmoid(1 * inner_product(w_params,n_train_list[j])))\n",
    "            \n",
    "            #ww ← w - η * (1 / n) * Σ_(n batch　のsum,ただしwは更新前に固定)_  (-y*x) / (1 + exp(y * <w,x>))\n",
    "            w_params = plus(w_params, cross(((-1) * learning_rate*(1/(batch_size*2))) , gradE_sums_for_minibatch ) )\n",
    "            Loss = Loss / batch_size\n",
    "            return w_params,Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train用とtest用に分ける\n",
    "with open(\"all_sentence_vectorised.vector\",\"rb\") as f:\n",
    "    alllist = pickle.load(f)\n",
    "    positive_test_list = []\n",
    "    negative_test_list = []\n",
    "    \n",
    "    for i in range(800,1000):\n",
    "        positive_test_list.append(alllist[0][i])\n",
    "        negative_test_list.append(alllist[1][i])\n",
    "    with open(\"./positive_test_list\",\"wb\") as f:\n",
    "        pickle.dump(positive_test_list,f)\n",
    "    with open(\"./negative_test_list\",\"wb\") as f:\n",
    "        pickle.dump(negative_test_list,f)\n",
    "        \n",
    "with open(\"all_sentence_vectorised.vector\",\"rb\") as f:\n",
    "    alllist = pickle.load(f)\n",
    "    positive_train_list = []\n",
    "    negative_train_list = []\n",
    "    for i in range(0,800):\n",
    "        positive_train_list.append(alllist[0][i])\n",
    "        negative_train_list.append(alllist[1][i])\n",
    "    with open(\"./positive_train_list\",\"wb\") as f:\n",
    "        pickle.dump(positive_train_list,f)\n",
    "    with open(\"./negative_train_list\",\"wb\") as f:\n",
    "        pickle.dump(negative_train_list,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1\n",
      "mean train loss per one sentence :0.1021375912486612 accuracy:50.0%\n",
      "epoch:2\n",
      "mean train loss per one sentence :0.1452557771278201 accuracy:50.0%\n",
      "epoch:3\n",
      "mean train loss per one sentence :0.09955320629221534 accuracy:50.0%\n",
      "epoch:4\n",
      "mean train loss per one sentence :0.07880551372286858 accuracy:50.0%\n",
      "epoch:5\n",
      "mean train loss per one sentence :0.11819803160712591 accuracy:50.0%\n",
      "epoch:6\n",
      "mean train loss per one sentence :0.09108242703154006 accuracy:50.0%\n",
      "epoch:7\n",
      "mean train loss per one sentence :0.034749343928006785 accuracy:50.0%\n",
      "epoch:8\n",
      "mean train loss per one sentence :0.07066471182477171 accuracy:50.0%\n",
      "epoch:9\n",
      "mean train loss per one sentence :0.0565607289567448 accuracy:50.0%\n",
      "epoch:10\n",
      "mean train loss per one sentence :0.04443005925204964 accuracy:50.0%\n"
     ]
    }
   ],
   "source": [
    "epoch_size = 10\n",
    "\n",
    "\n",
    "batch_size = 20\n",
    "#update_size = 300\n",
    "learning_rate = 10\n",
    "\n",
    "w = [random.gauss(0,1) for _ in range(200000)]\n",
    "b = [random.gauss(0,1) for _ in range(200000)]\n",
    "\n",
    "for  i in range(epoch_size):\n",
    "    print(\"epoch:\"+str(i + 1))\n",
    "    w,loss = training(w,batch_size,learning_rate)\n",
    "    loss_and_accuracy = minibatch_Loss_and_accuracy_calc_for_test(w,batch_size)\n",
    "    print(\"mean train loss per one sentence :\" + str(loss) + \" accuracy:\" + str(loss_and_accuracy[1] * 100) + \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.919793425831236\n",
      "2.6691290446120592\n",
      "-0.2871797321429228\n",
      "0.4159954583568954\n",
      "1.7673117875777495\n",
      "0.8185193260061636\n",
      "0.360663633200795\n",
      "1.549959322092778\n",
      "2.225265962381628\n",
      "1.4340999871690685\n",
      "1.9885615276220303\n",
      "1.2186110572784095\n",
      "7.014179177524806\n",
      "0.20160773361046078\n",
      "1.4304990915985032\n",
      "2.776014301027001\n",
      "0.479292202176563\n",
      "-1.4677021591799742\n",
      "-0.5434916759441396\n",
      "0.2075112480848381\n",
      "1.33229773520269\n",
      "2.305844648915583\n",
      "0.48114048533910847\n",
      "2.0758241751347493\n",
      "5.484272062024702\n",
      "1.6884717168235746\n",
      "2.2605117335520495\n",
      "8.772637483508225\n",
      "1.9969479911835841\n",
      "2.0148440232193012\n",
      "2.1474311714479906\n",
      "-0.05875576594251464\n",
      "3.667982677453009\n",
      "0.21981465858657873\n",
      "0.11893398552386944\n",
      "2.050858815497126\n",
      "3.0115858210219737\n",
      "-0.08068302123496901\n",
      "0.2598673562579338\n",
      "3.10645049807493\n",
      "0.5851127986674504\n",
      "1.7447591905219737\n",
      "2.4493726081274123\n",
      "-0.09602155629102563\n",
      "-1.421720412319179\n",
      "2.1388433770760376\n",
      "0.4473940123889819\n",
      "4.85168001539476\n",
      "1.497595253381068\n",
      "1.734279934350774\n",
      "1.3792276797159522\n",
      "1.942920440916295\n",
      "6.449136452103497\n",
      "2.197506164299638\n",
      "0.05004005310055448\n",
      "3.497744133081496\n",
      "18.03382986120147\n",
      "0.12052117937959872\n",
      "0.053020269109302486\n",
      "1.0963017814442755\n",
      "1.291703400691979\n",
      "0.6890209911577239\n",
      "7.537319642024005\n",
      "1.2821284996067288\n",
      "1.4429723199096098\n",
      "3.0828607515075848\n",
      "8.392948374588\n",
      "0.25740211497373167\n",
      "1.2293891914655504\n",
      "2.19907976519735\n",
      "2.6705133857233068\n",
      "1.6032756063633868\n",
      "1.2281638565529658\n",
      "2.415888304351459\n",
      "7.04004459385936\n",
      "1.6874777230911802\n",
      "3.022709741525271\n",
      "0.743631329193928\n",
      "1.1201087265486198\n",
      "2.1034716549446624\n",
      "2.410147730209753\n",
      "1.0609010762023714\n",
      "-0.20437767142156885\n",
      "0.5957289919326576\n",
      "0.7033002301899216\n",
      "-0.1225866497613452\n",
      "0.415097099351707\n",
      "-0.35313489500980055\n",
      "2.0133886193513915\n",
      "1.5452690424613378\n",
      "2.8258256039017207\n",
      "1.5996206292668829\n",
      "2.409584685596666\n",
      "2.09686296255736\n",
      "1.5690919054008865\n",
      "1.3957368590247052\n",
      "2.59053009431722\n",
      "1.6644712683826501\n",
      "1.1475411290231246\n",
      "0.7281889279217122\n"
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    print(w[i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

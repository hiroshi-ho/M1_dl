{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 sentence is vectorised\n",
      "500 sentence is vectorised\n",
      "0 sentence is vectorised\n",
      "500 sentence is vectorised\n",
      "2000 sentence is vectorised\n"
     ]
    }
   ],
   "source": [
    "#データをベクトル化する\n",
    "#かつ、データについてshuffleする。\n",
    "# -*- coding: utf-8 -*-\n",
    "import math\n",
    "import csv\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "shuffle_repeat = 100\n",
    "\n",
    "def sentence_vectorize():\n",
    "    \n",
    "    #positive,negativeすべて合わせた文のインデックス用\n",
    "    #後でシャッフル呼び出し時に用いる\n",
    "    total_index = 0\n",
    "    \n",
    "    #positive,negative両方用\n",
    "    all_sentence_vectorized = []\n",
    "    #positive : sign =1, negative : sign = -1\n",
    "    with open(\"./data/books/positive.review\",mode = \"r\", encoding = \"utf-8\") as p:\n",
    "        index = 0   #各文にインデックスを付ける。シャッフル呼び出し用\n",
    "        positive = []\n",
    "        for line in p:\n",
    "            \n",
    "            oneline = line.split(\" \") #まだ　\" \" でsplitしただけ\n",
    "            one_sentence = [0 for _ in range(200000)]  #これから一文をベクトル化する\n",
    "            \n",
    "            #一文内の各id:countに対する処理\n",
    "            for one_id_count_set in oneline:\n",
    "                id_,count_ = one_id_count_set.split(\":\")\n",
    "                # 文末のid:count用\n",
    "                count_ = count_.strip() \n",
    "                \n",
    "                one_sentence[int(id_)] += int(count_) \n",
    "\n",
    "            #一文に対する処理　ここまで\n",
    "            #one_sentenceを記録する、ただし、sign,indexをつける必要がある\n",
    "            \n",
    "            #sign, index用も込のリスト、すなわち\n",
    "            #[ sign ,  index,  [ 200000次元のリスト ]]　とする。\n",
    "            \n",
    "            \n",
    "            \n",
    "            if index % 500 == 0:\n",
    "                print(str(index) + \" sentence is vectorised\")\n",
    "            \n",
    "            positive.append(one_sentence)\n",
    "                \n",
    "            #各sentenceに固有\n",
    "            index +=1\n",
    "        # shuffle  \n",
    "        for i in range(shuffle_repeat):\n",
    "            positive = random.sample(positive,len(positive))\n",
    "        with open(\"./positive.review.vector\",\"wb\") as pp:\n",
    "            pickle.dump(positive,pp)\n",
    "        \n",
    "        #negative_dataの各文章に対するインデックス開始は1から\n",
    "        #total_indexは全センテンス用\n",
    "        total_index += index\n",
    "        all_sentence_vectorized.append(positive)\n",
    "\n",
    "    with open(\"./data/books/negative.review\",mode = \"r\", encoding = \"utf-8\") as n:\n",
    "        index = 0   #各文にインデックスを付ける。シャッフル呼び出し用\n",
    "        negative = []\n",
    "        for line in n:\n",
    "            \n",
    "            oneline = line.split(\" \") #まだ　\" \" でsplitしただけ\n",
    "            one_sentence = [0 for _ in range(200000)]  #これから一文をベクトル化する\n",
    "            \n",
    "            #一文内の各id:countに対する処理\n",
    "            for one_id_count_set in oneline:\n",
    "                id_,count_ = one_id_count_set.split(\":\")\n",
    "                # 文末のid:count用\n",
    "                count_ = count_.strip() \n",
    "                \n",
    "                one_sentence[int(id_)] += int(count_) \n",
    "            #一文に対する処理　ここまで\n",
    "            #one_sentenceを記録する、ただし、sign,indexをつける必要がある\n",
    "            \n",
    "            #sign, index用も込のリスト、すなわち\n",
    "            #[ sign ,  index,  [ 200000次元のリスト ]]　とする。\n",
    "            \n",
    "            #sentence_to_be_recorded = [index,-1,one_sentence]\n",
    "            \n",
    "            if index % 500 == 0:\n",
    "                print(str(index) + \" sentence is vectorised\")\n",
    "            \n",
    "            negative.append(one_sentence)\n",
    "                \n",
    "            #各sentenceに固有\n",
    "            index +=1\n",
    "            total_index +=1 \n",
    "        for i in range(shuffle_repeat):\n",
    "            negative = random.sample(negative,len(negative))\n",
    "    all_sentence_vectorized.append(negative)\n",
    "    \n",
    "   \n",
    "    with open(\"./negative.review.vector\",\"wb\") as nn:\n",
    "            pickle.dump(negative,nn) \n",
    "            \n",
    "        \n",
    "    with open(\"./all_sentence_vectorised.vector\",\"wb\") as f:\n",
    "        pickle.dump(all_sentence_vectorized,f)\n",
    "        \n",
    "    print(str(total_index) + \" sentence is vectorised\")\n",
    "    \n",
    "sentence_vectorize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split train=0.8, dev = 0.1, test = 0.1\n",
    "#bias is needed\n",
    "\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "def data_spliter():\n",
    "    p_train = []\n",
    "    p_dev = []\n",
    "    p_test = []\n",
    "    \n",
    "    n_train = []\n",
    "    n_dev = []\n",
    "    n_test = []\n",
    "    \n",
    "    with open(\"positive.review.vector\",\"rb\") as p:\n",
    "        p_list = pickle.load(p)\n",
    "\n",
    "        for i in range(len( p_list)):\n",
    "            if i <800:\n",
    "                p_train.append(p_list[i])\n",
    "            elif i >= 800 and i <= 900:\n",
    "                p_dev.append(p_list[i])\n",
    "            else:\n",
    "                p_test.append(p_list[i])\n",
    "                \n",
    "    with open(\"negative.review.vector\",\"rb\") as n:\n",
    "        n_list = pickle.load(n)\n",
    "\n",
    "        for i in range(len(n_list)):\n",
    "            if i <800:\n",
    "                n_train.append(n_list[i])\n",
    "            elif i >= 800 and i < 900:\n",
    "                n_dev.append(n_list[i])\n",
    "            else:\n",
    "                n_test.append(n_list[i])    \n",
    "        \n",
    "    with open(\"./train_dev_test/p_train\",\"wb\") as f:\n",
    "        pickle.dump(p_train,f)\n",
    "\n",
    "    with open(\"./train_dev_test/p_dev\",\"wb\") as f:\n",
    "        pickle.dump(p_dev,f)\n",
    "\n",
    "    with open(\"./train_dev_test/p_test\",\"wb\") as f:\n",
    "        pickle.dump(p_test,f)\n",
    "        \n",
    "    with open(\"./train_dev_test/n_train\",\"wb\") as f:\n",
    "        pickle.dump(n_train,f)\n",
    "\n",
    "    with open(\"./train_dev_test/n_dev\",\"wb\") as f:\n",
    "        pickle.dump(n_dev,f)\n",
    "\n",
    "    with open(\"./train_dev_test/n_test\",\"wb\") as f:\n",
    "        pickle.dump(n_test,f)\n",
    "        \n",
    "data_spliter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function\n",
    "\n",
    "import math\n",
    "def sigmoid(xx):\n",
    "    if xx > 0:\n",
    "        return (1 / (1 + math.exp(-xx)))\n",
    "    else:\n",
    "        return (1 - 1 / (1 + math.exp(xx)))\n",
    "        \n",
    "\n",
    "def v_dot(list_x,list_y): #内積\n",
    "    sums = 0\n",
    "    for i in range(len(list_x)):\n",
    "        sums+= list_x[i] * list_y[i]\n",
    "    return sums\n",
    "\n",
    "def v_plus(list_x,list_y):\n",
    "    result = [0 for _ in range(len(list_x))]\n",
    "    for i in range(len(list_x)):\n",
    "        result[i] = list_x[i] + list_y[i]\n",
    "    return result    \n",
    "\n",
    "def v_minus(list_x,list_y):\n",
    "    result = [0 for _ in range(len(list_x))]\n",
    "    for i in range(len(list_x)):\n",
    "        result[i] = list_x[i] - list_y[i]\n",
    "    return result    \n",
    "\n",
    "def v_a_fold(a,list_x): #vectorの定数倍\n",
    "    for i in list_x:\n",
    "        i = a * i\n",
    "    return list_x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([965, 521, 923, 370, 673, 279, 902, 644, 156, 946, 854, 209, 203,\n",
       "       207, 441, 157, 830, 321, 257, 330])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.choice([i for i in range(1000)],20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "def load_train(batch_size):\n",
    "    p_ret = []\n",
    "    n_ret = []\n",
    "    p_index = np.random.choice([i for i in range(800)],batch_size)\n",
    "    n_index = np.random.choice([i for i in range(800)],batch_size)\n",
    "    with open(\"./train_dev_test/p_train\",\"rb\") as p:\n",
    "        with open(\"./train_dev_test/n_train\",\"rb\") as n:\n",
    "            p_train = pickle.load(p)\n",
    "            n_train = pickle.load(n)\n",
    "            \n",
    "            for _ in p_index:\n",
    "                p_ret.append(p_train[_])\n",
    "                \n",
    "            for _ in n_index:\n",
    "                n_ret.append(n_train[_])\n",
    "    return p_ret, n_ret\n",
    "\n",
    "\n",
    "\n",
    "def training(w,b,p_batch,n_batch,learning_rate,mini_batch_size):\n",
    "    ips = 10 ** (-7)\n",
    "    \n",
    "    loss = 0\n",
    "    one_iter = np.array( [0 for i in range(200000)])\n",
    "    one_b = 0\n",
    "    \n",
    "    # 1 epochの中でp_batch(size = load_batch), n_batch(size = load_batch) 取ってきて\n",
    "    #　その中からランダムで　mini_batch　個のデータで　1epoch回す。\n",
    "    \n",
    "    \"\"\"\n",
    "    for batch in p_train_batchs :\n",
    "        y = 1\n",
    "        one_iter = v_plus(one_iter ,  v_a_fold(((-1) * (y) * sigmoid((-1) * y * np.dot(w,batch))),batch) )\n",
    "        one_b = one_b + (-1) * y * 1 * sigmoid((-1) * y * np.dot(w,batch))\n",
    "        loss += (-1) * math.log(ips + sigmoid(y * np.dot(w,one_iter) + b))    \n",
    "        #w = np.add(w ,(-1)* (learning_rate / len(batch)) * one_iter)\n",
    "        \n",
    "    w = np.add(w, v_a_fold((-1)* (learning_rate / len(p_train_batchs)) , one_iter) )\n",
    "    #b = np.add( b,  ((-1)* (learning_rate / len(batch)) )* one_b)\n",
    "    bias = bias - ((-1)* (learning_rate / len(p_train_batchs)) )* one_b      \n",
    "    print(\"Loss: \" + str(loss))\n",
    "    \n",
    "    loss = 0\n",
    "    one_iter = np.array( [0 for i in range(200000)])\n",
    "    one_b = 0\n",
    "    for batch in n_train_batchs :\n",
    "        y = -1\n",
    "        one_iter = v_plus(one_iter ,  v_a_fold(((-1) * (y) * sigmoid((-1) * y * np.dot(w,batch))),batch) )\n",
    "        one_b = one_b + (-1) * y * 1 * sigmoid((-1) * y * np.dot(w,batch))\n",
    "        loss += (-1) * math.log(ips + sigmoid(y * np.dot(w,one_iter) + b))    \n",
    "    #w = np.add(w ,(-1)* (learning_rate / len(batch)) * one_iter) \n",
    "    w = np.add(w, v_a_fold((-1)* (learning_rate / len(n_train_batchs)) , one_iter) )\n",
    "    #b = np.add( b,  ((-1)* (learning_rate / len(batch)) )* one_b)\n",
    "    bias = bias - ((-1)* (learning_rate / len(n_train_batchs)) )* one_b      \n",
    "    print(\"Loss: \" + str(loss))    \n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    #print(\"p_batch\"+str(p_batch))\n",
    "\n",
    "    indexed_sentence = []\n",
    "    for sentence in p_batch:\n",
    "        indexed_sentence.append([1,sentence])\n",
    "        \n",
    "    for sentence in n_batch:\n",
    "        indexed_sentence.append([-1,sentence])\n",
    "        \n",
    "    batch_to_be_used_index = np.random.choice(len(indexed_sentence),mini_batch_size)\n",
    "    batch_to_be_used = []\n",
    "    for _ in batch_to_be_used_index:\n",
    "        batch_to_be_used.append(indexed_sentence[_])\n",
    "    \n",
    "    #print(batch_to_be_used)\n",
    "    Loss = 0\n",
    "    delL_devide_delw = np.array([0 for _ in range(200000)])\n",
    "    delL_devide_delw_for_b = 0\n",
    "    \n",
    "    \"\"\"\n",
    "    delL_devide_delw = -y * sigmoid(-y *( np.dot(w,x) + b)) * x\n",
    "    delL_devide_delw_for_b  = (-y) * 1 * sigmoid(-y *( np.dot(w,x) + b))\n",
    "    \n",
    "    \"\"\"\n",
    "    #w,b  : fixed\n",
    "    for pn_sentence in batch_to_be_used:\n",
    "        y = pn_sentence[0]\n",
    "        #print(y)\n",
    "        x = pn_sentence[1]\n",
    "        x = np.array(x)\n",
    "        #print(x,len(w))\n",
    "        delL_devide_delw = np.array(delL_devide_delw) + (-1) * y * sigmoid((-1) * (y) * (np.dot(w,x)) )* x\n",
    "        delL_devide_delw_for_b += (-1) * y * sigmoid((-1) * (y) * (np.dot(w,x)) )* 1 # x内に成分1がappendされて,wxを考える\n",
    "        Loss += (-1) * math.log(ips +sigmoid(y * np.dot(w,x)))\n",
    "    #learning\n",
    "    w = w - ( learning_rate / len(batch_to_be_used) ) * delL_devide_delw\n",
    "    b  =  b -  ( learning_rate / len(batch_to_be_used) ) * delL_devide_delw_for_b\n",
    "    Loss = Loss / len(batch_to_be_used)\n",
    "    \n",
    "    return w,b,Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dev():\n",
    "    p_dev = []\n",
    "    n_dev = []\n",
    "    with open(\"./train_dev_test/p_dev\",\"rb\") as p:\n",
    "        with open(\"./train_dev_test/n_dev\",\"rb\") as n:\n",
    "            p_dev = pickle.load(p)\n",
    "            n_dev = pickle.load(n)\n",
    "    return p_dev, n_dev\n",
    "    \n",
    "def accuracy_eval(w,b):\n",
    "    p_dev,n_dev = load_dev()\n",
    "    correct = 0\n",
    "    all_count = 0\n",
    "    \n",
    "    for _ in p_dev:\n",
    "        all_count += 1\n",
    "        y_ = 1\n",
    "        _ = np.array(_)\n",
    "        if np.dot(w,_) + b >=0:\n",
    "            correct += 1\n",
    "    for _ in n_dev:\n",
    "        all_count += 1\n",
    "        y_ = -1\n",
    "        _ = np.array(_)\n",
    "        if ((np.dot(w,_) + b)) <0:\n",
    "            correct += 1    \n",
    "    accuracy = correct * 100 / all_count\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch1 end\n",
      "Loss: 5.619368818465969\n",
      "accuracy: 46.26865671641791\n",
      "epoch2 end\n",
      "Loss: 5.378997360930482\n",
      "accuracy: 53.233830845771145\n",
      "epoch3 end\n",
      "Loss: 4.572110591547991\n",
      "accuracy: 51.74129353233831\n",
      "epoch4 end\n",
      "Loss: 3.8163068780266163\n",
      "accuracy: 52.7363184079602\n",
      "epoch5 end\n",
      "Loss: 2.8882080422427805\n",
      "accuracy: 52.23880597014925\n",
      "epoch6 end\n",
      "Loss: 2.073520787379147\n",
      "accuracy: 52.23880597014925\n",
      "epoch7 end\n",
      "Loss: 2.896409928799325\n",
      "accuracy: 50.24875621890547\n",
      "epoch8 end\n",
      "Loss: 1.7176639368263862\n",
      "accuracy: 50.24875621890547\n",
      "epoch9 end\n",
      "Loss: 1.98596715750442\n",
      "accuracy: 49.75124378109453\n",
      "epoch10 end\n",
      "Loss: 1.0210949874442974\n",
      "accuracy: 49.75124378109453\n",
      "epoch11 end\n",
      "Loss: 2.0478134335492815\n",
      "accuracy: 49.75124378109453\n",
      "epoch12 end\n",
      "Loss: 1.6704423176475647\n",
      "accuracy: 49.75124378109453\n",
      "epoch13 end\n",
      "Loss: 1.4113020915348071\n",
      "accuracy: 49.75124378109453\n",
      "epoch14 end\n",
      "Loss: 1.1485173383823324\n",
      "accuracy: 49.75124378109453\n",
      "epoch15 end\n",
      "Loss: 1.8576016365140111\n",
      "accuracy: 49.75124378109453\n",
      "epoch16 end\n",
      "Loss: 1.3308721503477414\n",
      "accuracy: 49.75124378109453\n",
      "epoch17 end\n",
      "Loss: 1.0932575757540868\n",
      "accuracy: 49.75124378109453\n",
      "epoch18 end\n",
      "Loss: 1.213880008151813\n",
      "accuracy: 49.75124378109453\n",
      "epoch19 end\n",
      "Loss: 1.3036280139916472\n",
      "accuracy: 49.75124378109453\n",
      "epoch20 end\n",
      "Loss: 1.6018412395530803\n",
      "accuracy: 49.75124378109453\n",
      "epoch21 end\n",
      "Loss: 1.054919596069287\n",
      "accuracy: 49.75124378109453\n",
      "epoch22 end\n",
      "Loss: 0.9542832107153795\n",
      "accuracy: 49.75124378109453\n",
      "epoch23 end\n",
      "Loss: 1.7373226488774325\n",
      "accuracy: 49.75124378109453\n",
      "epoch24 end\n",
      "Loss: 1.1669933327428423\n",
      "accuracy: 49.75124378109453\n",
      "epoch25 end\n",
      "Loss: 1.935174425455975\n",
      "accuracy: 49.75124378109453\n",
      "epoch26 end\n",
      "Loss: 0.5479583964050585\n",
      "accuracy: 49.75124378109453\n",
      "epoch27 end\n",
      "Loss: 0.9987722110273342\n",
      "accuracy: 49.75124378109453\n",
      "epoch28 end\n",
      "Loss: 1.2873452565013912\n",
      "accuracy: 49.75124378109453\n",
      "epoch29 end\n",
      "Loss: 0.7691864385714939\n",
      "accuracy: 49.75124378109453\n",
      "epoch30 end\n",
      "Loss: 1.4893840298639702\n",
      "accuracy: 49.75124378109453\n",
      "epoch31 end\n",
      "Loss: 0.7055751387373317\n",
      "accuracy: 49.75124378109453\n",
      "epoch32 end\n",
      "Loss: 0.8985940414343577\n",
      "accuracy: 49.75124378109453\n",
      "epoch33 end\n",
      "Loss: 0.5512377841762154\n",
      "accuracy: 49.75124378109453\n",
      "epoch34 end\n",
      "Loss: 0.6493289556665311\n",
      "accuracy: 49.75124378109453\n",
      "epoch35 end\n",
      "Loss: 1.1955022515012927\n",
      "accuracy: 49.75124378109453\n",
      "epoch36 end\n",
      "Loss: 1.0313243928922713\n",
      "accuracy: 49.75124378109453\n",
      "epoch37 end\n",
      "Loss: 1.0564383477369481\n",
      "accuracy: 49.75124378109453\n",
      "epoch38 end\n",
      "Loss: 0.5912921813222424\n",
      "accuracy: 49.75124378109453\n",
      "epoch39 end\n",
      "Loss: 0.7387522780262978\n",
      "accuracy: 49.75124378109453\n",
      "epoch40 end\n",
      "Loss: 0.7773914760858127\n",
      "accuracy: 49.75124378109453\n",
      "epoch41 end\n",
      "Loss: 0.5556853582945909\n",
      "accuracy: 49.75124378109453\n",
      "epoch42 end\n",
      "Loss: 0.3578943028086909\n",
      "accuracy: 49.75124378109453\n",
      "epoch43 end\n",
      "Loss: 0.7574969620159814\n",
      "accuracy: 49.75124378109453\n",
      "epoch44 end\n",
      "Loss: 0.4553191784876551\n",
      "accuracy: 49.75124378109453\n",
      "epoch45 end\n",
      "Loss: 0.421829056557928\n",
      "accuracy: 49.75124378109453\n",
      "epoch46 end\n",
      "Loss: 0.7311978406921991\n",
      "accuracy: 49.75124378109453\n",
      "epoch47 end\n",
      "Loss: 0.5860957529099668\n",
      "accuracy: 49.75124378109453\n",
      "epoch48 end\n",
      "Loss: 0.6320860397151382\n",
      "accuracy: 49.75124378109453\n",
      "epoch49 end\n",
      "Loss: 0.7985048960078603\n",
      "accuracy: 49.75124378109453\n",
      "epoch50 end\n",
      "Loss: 0.6399939859256698\n",
      "accuracy: 49.75124378109453\n",
      "epoch51 end\n",
      "Loss: 0.8096938833006749\n",
      "accuracy: 49.75124378109453\n",
      "epoch52 end\n",
      "Loss: 0.7806195128809038\n",
      "accuracy: 49.75124378109453\n",
      "epoch53 end\n",
      "Loss: 0.4803100639112616\n",
      "accuracy: 49.75124378109453\n",
      "epoch54 end\n",
      "Loss: 0.421162969092527\n",
      "accuracy: 49.75124378109453\n",
      "epoch55 end\n",
      "Loss: 0.5785635756217364\n",
      "accuracy: 49.75124378109453\n",
      "epoch56 end\n",
      "Loss: 0.9430434715872869\n",
      "accuracy: 49.75124378109453\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-3e4b0a135347>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0macc_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch_times\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mp_trainbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_trainbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mload_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0mw_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLoss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw_tr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp_trainbatch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_trainbatch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmini_batch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"epoch\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" end\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-85e66c3041a2>\u001b[0m in \u001b[0;36mload_train\u001b[0;34m(batch_size)\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./train_dev_test/n_train_div100\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0mp_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m             \u001b[0mn_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mp_index\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "epoch_times =400\n",
    "\n",
    "load_batch = 100\n",
    "mini_batch_size = 10\n",
    "learning_rate = 0.1\n",
    "\n",
    "w_tr = np.random.rand(200000)\n",
    "b_tr = np.random.rand()\n",
    "\n",
    "Loss_list = []\n",
    "acc_list = []\n",
    "for _ in range(epoch_times):\n",
    "    p_trainbatch, n_trainbatch = load_train(load_batch)\n",
    "    w_, b_, Loss = training(w_tr,b_tr, p_trainbatch,n_trainbatch,learning_rate,mini_batch_size)\n",
    "    print(\"epoch\" + str(_ + 1) + \" end\")\n",
    "    print(\"Loss: \"+str(Loss))\n",
    "    accuracy = accuracy_eval( w_, b_ )\n",
    "    print(\"accuracy: \"+str(accuracy))\n",
    "    w_tr = w_\n",
    "    b_tr = b_\n",
    "    Loss_list.append(Loss)\n",
    "    acc_list.append(accuracy)\n",
    "    \n",
    "with open(\"./w_record_mb10_lr0p1\",\"wb\") as f:\n",
    "    pickle.dump(w_tr,f)\n",
    "    \n",
    "with open(\"./b_record_mb10_lr0p1\",\"wb\") as f:\n",
    "    pickle.dump(b_tr,f)\n",
    "    \n",
    "with open(\"./record/acc_record_mb10_lr0p1\",\"wb\") as f:\n",
    "    pickle.dump(acc_list,f)\n",
    "    \n",
    "with open(\"./record/loss_record_mb10_lr0p1\",\"wb\") as f:\n",
    "    pickle.dump(Loss_list,f)\n",
    "    \n",
    "plt.plot(Loss_list)\n",
    "plt.show()#Loss\n",
    "print()\n",
    "plt.figure()\n",
    "plt.plot(accuracy_list)\n",
    "plt.show()\n",
    "#accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "epoch_times =400\n",
    "\n",
    "load_batch = 100\n",
    "mini_batch_size = 10\n",
    "learning_rate = 0.01\n",
    "\n",
    "w_tr = np.random.rand(200000)\n",
    "b_tr = np.random.rand()\n",
    "\n",
    "Loss_list = []\n",
    "acc_list = []\n",
    "for _ in range(epoch_times):\n",
    "    p_trainbatch, n_trainbatch = load_train(load_batch)\n",
    "    w_, b_, Loss = training(w_tr,b_tr, p_trainbatch,n_trainbatch,learning_rate,mini_batch_size)\n",
    "    print(\"epoch\" + str(_ + 1) + \" end\")\n",
    "    print(\"Loss: \"+str(Loss))\n",
    "    accuracy = accuracy_eval( w_, b_ )\n",
    "    print(\"accuracy: \"+str(accuracy))\n",
    "    w_tr = w_\n",
    "    b_tr = b_\n",
    "    Loss_list.append(Loss)\n",
    "    acc_list.append(accuracy)\n",
    "    \n",
    "with open(\"./w_record_mb10_lr0p01\",\"wb\") as f:\n",
    "    pickle.dump(w_tr,f)\n",
    "    \n",
    "with open(\"./b_record_mb10_lr0p01\",\"wb\") as f:\n",
    "    pickle.dump(b_tr,f)\n",
    "    \n",
    "with open(\"./record/acc_record_mb10_lr0p01\",\"wb\") as f:\n",
    "    pickle.dump(acc_list,f)\n",
    "    \n",
    "with open(\"./record/loss_record_mb10_lr0p01\",\"wb\") as f:\n",
    "    pickle.dump(Loss_list,f)\n",
    "    \n",
    "plt.plot(Loss_list)\n",
    "plt.show()#Loss\n",
    "print()\n",
    "plt.figure()\n",
    "plt.plot(accuracy_list)\n",
    "plt.show()\n",
    "#accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

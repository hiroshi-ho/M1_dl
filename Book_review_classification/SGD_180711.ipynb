{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 sentence is vectorised\n",
      "500 sentence is vectorised\n",
      "0 sentence is vectorised\n",
      "500 sentence is vectorised\n",
      "2000 sentence is vectorised\n"
     ]
    }
   ],
   "source": [
    "#データをベクトル化する\n",
    "#かつ、データについてshuffleする。\n",
    "# -*- coding: utf-8 -*-\n",
    "import math\n",
    "import csv\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "shuffle_repeat = 100\n",
    "\n",
    "def sentence_vectorize():\n",
    "    \n",
    "    #positive,negativeすべて合わせた文のインデックス用\n",
    "    #後でシャッフル呼び出し時に用いる\n",
    "    total_index = 0\n",
    "    \n",
    "    #positive,negative両方用\n",
    "    all_sentence_vectorized = []\n",
    "    #positive : sign =1, negative : sign = -1\n",
    "    with open(\"./data/books/positive.review\",mode = \"r\", encoding = \"utf-8\") as p:\n",
    "        index = 0   #各文にインデックスを付ける。シャッフル呼び出し用\n",
    "        positive = []\n",
    "        for line in p:\n",
    "            \n",
    "            oneline = line.split(\" \") #まだ　\" \" でsplitしただけ\n",
    "            one_sentence = [0 for _ in range(200000)]  #これから一文をベクトル化する\n",
    "            \n",
    "            #一文内の各id:countに対する処理\n",
    "            for one_id_count_set in oneline:\n",
    "                id_,count_ = one_id_count_set.split(\":\")\n",
    "                # 文末のid:count用\n",
    "                count_ = count_.strip() \n",
    "                \n",
    "                one_sentence[int(id_)] += int(count_) \n",
    "\n",
    "            #一文に対する処理　ここまで\n",
    "            #one_sentenceを記録する、ただし、sign,indexをつける必要がある\n",
    "            \n",
    "            #sign, index用も込のリスト、すなわち\n",
    "            #[ sign ,  index,  [ 200000次元のリスト ]]　とする。\n",
    "            \n",
    "            \n",
    "            \n",
    "            if index % 500 == 0:\n",
    "                print(str(index) + \" sentence is vectorised\")\n",
    "            \n",
    "            positive.append(one_sentence)\n",
    "                \n",
    "            #各sentenceに固有\n",
    "            index +=1\n",
    "        # shuffle  \n",
    "        for i in range(shuffle_repeat):\n",
    "            positive = random.sample(positive,len(positive))\n",
    "        with open(\"./positive.review.vector\",\"wb\") as pp:\n",
    "            pickle.dump(positive,pp)\n",
    "        \n",
    "        #negative_dataの各文章に対するインデックス開始は1から\n",
    "        #total_indexは全センテンス用\n",
    "        total_index += index\n",
    "        all_sentence_vectorized.append(positive)\n",
    "\n",
    "    with open(\"./data/books/negative.review\",mode = \"r\", encoding = \"utf-8\") as n:\n",
    "        index = 0   #各文にインデックスを付ける。シャッフル呼び出し用\n",
    "        negative = []\n",
    "        for line in n:\n",
    "            \n",
    "            oneline = line.split(\" \") #まだ　\" \" でsplitしただけ\n",
    "            one_sentence = [0 for _ in range(200000)]  #これから一文をベクトル化する\n",
    "            \n",
    "            #一文内の各id:countに対する処理\n",
    "            for one_id_count_set in oneline:\n",
    "                id_,count_ = one_id_count_set.split(\":\")\n",
    "                # 文末のid:count用\n",
    "                count_ = count_.strip() \n",
    "                \n",
    "                one_sentence[int(id_)] += int(count_) \n",
    "            #一文に対する処理　ここまで\n",
    "            #one_sentenceを記録する、ただし、sign,indexをつける必要がある\n",
    "            \n",
    "            #sign, index用も込のリスト、すなわち\n",
    "            #[ sign ,  index,  [ 200000次元のリスト ]]　とする。\n",
    "            \n",
    "            #sentence_to_be_recorded = [index,-1,one_sentence]\n",
    "            \n",
    "            if index % 500 == 0:\n",
    "                print(str(index) + \" sentence is vectorised\")\n",
    "            \n",
    "            negative.append(one_sentence)\n",
    "                \n",
    "            #各sentenceに固有\n",
    "            index +=1\n",
    "            total_index +=1 \n",
    "        for i in range(shuffle_repeat):\n",
    "            negative = random.sample(negative,len(negative))\n",
    "    all_sentence_vectorized.append(negative)\n",
    "    \n",
    "   \n",
    "    with open(\"./negative.review.vector\",\"wb\") as nn:\n",
    "            pickle.dump(negative,nn) \n",
    "            \n",
    "        \n",
    "    with open(\"./all_sentence_vectorised.vector\",\"wb\") as f:\n",
    "        pickle.dump(all_sentence_vectorized,f)\n",
    "        \n",
    "    print(str(total_index) + \" sentence is vectorised\")\n",
    "    \n",
    "sentence_vectorize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split train=0.8, dev = 0.1, test = 0.1\n",
    "#bias is needed\n",
    "\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "def data_spliter():\n",
    "    p_train = []\n",
    "    p_dev = []\n",
    "    p_test = []\n",
    "    \n",
    "    n_train = []\n",
    "    n_dev = []\n",
    "    n_test = []\n",
    "    \n",
    "    with open(\"positive.review.vector\",\"rb\") as p:\n",
    "        p_list = pickle.load(p)\n",
    "\n",
    "        for i in range(len( p_list)):\n",
    "            if i <800:\n",
    "                p_train.append(p_list[i])\n",
    "            elif i >= 800 and i <= 900:\n",
    "                p_dev.append(p_list[i])\n",
    "            else:\n",
    "                p_test.append(p_list[i])\n",
    "                \n",
    "    with open(\"negative.review.vector\",\"rb\") as n:\n",
    "        n_list = pickle.load(n)\n",
    "\n",
    "        for i in range(len(n_list)):\n",
    "            if i <800:\n",
    "                n_train.append(n_list[i])\n",
    "            elif i >= 800 and i < 900:\n",
    "                n_dev.append(n_list[i])\n",
    "            else:\n",
    "                n_test.append(n_list[i])    \n",
    "        \n",
    "    with open(\"./train_dev_test/p_train\",\"wb\") as f:\n",
    "        pickle.dump(p_train,f)\n",
    "\n",
    "    with open(\"./train_dev_test/p_dev\",\"wb\") as f:\n",
    "        pickle.dump(p_dev,f)\n",
    "\n",
    "    with open(\"./train_dev_test/p_test\",\"wb\") as f:\n",
    "        pickle.dump(p_test,f)\n",
    "        \n",
    "    with open(\"./train_dev_test/n_train\",\"wb\") as f:\n",
    "        pickle.dump(n_train,f)\n",
    "\n",
    "    with open(\"./train_dev_test/n_dev\",\"wb\") as f:\n",
    "        pickle.dump(n_dev,f)\n",
    "\n",
    "    with open(\"./train_dev_test/n_test\",\"wb\") as f:\n",
    "        pickle.dump(n_test,f)\n",
    "        \n",
    "data_spliter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function\n",
    "\n",
    "import math\n",
    "def sigmoid(xx):\n",
    "    if xx > 0:\n",
    "        return (1 / (1 + math.exp(-xx)))\n",
    "    else:\n",
    "        return (1 - 1 / (1 + math.exp(xx)))\n",
    "        \n",
    "\n",
    "def v_dot(list_x,list_y): #内積\n",
    "    sums = 0\n",
    "    for i in range(len(list_x)):\n",
    "        sums+= list_x[i] * list_y[i]\n",
    "    return sums\n",
    "\n",
    "def v_plus(list_x,list_y):\n",
    "    result = [0 for _ in range(len(list_x))]\n",
    "    for i in range(len(list_x)):\n",
    "        result[i] = list_x[i] + list_y[i]\n",
    "    return result    \n",
    "\n",
    "def v_minus(list_x,list_y):\n",
    "    result = [0 for _ in range(len(list_x))]\n",
    "    for i in range(len(list_x)):\n",
    "        result[i] = list_x[i] - list_y[i]\n",
    "    return result    \n",
    "\n",
    "def v_a_fold(a,list_x): #vectorの定数倍\n",
    "    for i in list_x:\n",
    "        i = a * i\n",
    "    return list_x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([965, 521, 923, 370, 673, 279, 902, 644, 156, 946, 854, 209, 203,\n",
       "       207, 441, 157, 830, 321, 257, 330])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.choice([i for i in range(1000)],20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "def load_train(batch_size):\n",
    "    p_ret = []\n",
    "    n_ret = []\n",
    "    p_index = np.random.choice([i for i in range(800)],batch_size)\n",
    "    n_index = np.random.choice([i for i in range(800)],batch_size)\n",
    "    with open(\"./train_dev_test/p_train\",\"rb\") as p:\n",
    "        with open(\"./train_dev_test/n_train\",\"rb\") as n:\n",
    "            p_train = pickle.load(p)\n",
    "            n_train = pickle.load(n)\n",
    "            \n",
    "            for _ in p_index:\n",
    "                p_ret.append(p_train[_])\n",
    "                \n",
    "            for _ in n_index:\n",
    "                n_ret.append(n_train[_])\n",
    "    return p_ret, n_ret\n",
    "\n",
    "\n",
    "\n",
    "def training(w,b,p_batch,n_batch,learning_rate,mini_batch_size):\n",
    "    ips = 10 ** (-7)\n",
    "    \n",
    "    loss = 0\n",
    "    one_iter = np.array( [0 for i in range(200000)])\n",
    "    one_b = 0\n",
    "    \n",
    "    # 1 epochの中でp_batch(size = load_batch), n_batch(size = load_batch) 取ってきて\n",
    "    #　その中からランダムで　mini_batch　個のデータで　1epoch回す。\n",
    "    \n",
    "    \"\"\"\n",
    "    for batch in p_train_batchs :\n",
    "        y = 1\n",
    "        one_iter = v_plus(one_iter ,  v_a_fold(((-1) * (y) * sigmoid((-1) * y * np.dot(w,batch))),batch) )\n",
    "        one_b = one_b + (-1) * y * 1 * sigmoid((-1) * y * np.dot(w,batch))\n",
    "        loss += (-1) * math.log(ips + sigmoid(y * np.dot(w,one_iter) + b))    \n",
    "        #w = np.add(w ,(-1)* (learning_rate / len(batch)) * one_iter)\n",
    "        \n",
    "    w = np.add(w, v_a_fold((-1)* (learning_rate / len(p_train_batchs)) , one_iter) )\n",
    "    #b = np.add( b,  ((-1)* (learning_rate / len(batch)) )* one_b)\n",
    "    bias = bias - ((-1)* (learning_rate / len(p_train_batchs)) )* one_b      \n",
    "    print(\"Loss: \" + str(loss))\n",
    "    \n",
    "    loss = 0\n",
    "    one_iter = np.array( [0 for i in range(200000)])\n",
    "    one_b = 0\n",
    "    for batch in n_train_batchs :\n",
    "        y = -1\n",
    "        one_iter = v_plus(one_iter ,  v_a_fold(((-1) * (y) * sigmoid((-1) * y * np.dot(w,batch))),batch) )\n",
    "        one_b = one_b + (-1) * y * 1 * sigmoid((-1) * y * np.dot(w,batch))\n",
    "        loss += (-1) * math.log(ips + sigmoid(y * np.dot(w,one_iter) + b))    \n",
    "    #w = np.add(w ,(-1)* (learning_rate / len(batch)) * one_iter) \n",
    "    w = np.add(w, v_a_fold((-1)* (learning_rate / len(n_train_batchs)) , one_iter) )\n",
    "    #b = np.add( b,  ((-1)* (learning_rate / len(batch)) )* one_b)\n",
    "    bias = bias - ((-1)* (learning_rate / len(n_train_batchs)) )* one_b      \n",
    "    print(\"Loss: \" + str(loss))    \n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    #print(\"p_batch\"+str(p_batch))\n",
    "\n",
    "    indexed_sentence = []\n",
    "    for sentence in p_batch:\n",
    "        indexed_sentence.append([1,sentence])\n",
    "        \n",
    "    for sentence in n_batch:\n",
    "        indexed_sentence.append([-1,sentence])\n",
    "        \n",
    "    batch_to_be_used_index = np.random.choice(len(indexed_sentence),mini_batch_size)\n",
    "    batch_to_be_used = []\n",
    "    for _ in batch_to_be_used_index:\n",
    "        batch_to_be_used.append(indexed_sentence[_])\n",
    "    \n",
    "    #print(batch_to_be_used)\n",
    "    Loss = 0\n",
    "    delL_devide_delw = np.array([0 for _ in range(200000)])\n",
    "    delL_devide_delw_for_b = 0\n",
    "    \n",
    "    \"\"\"\n",
    "    delL_devide_delw = -y * sigmoid(-y *( np.dot(w,x) + b)) * x\n",
    "    delL_devide_delw_for_b  = (-y) * 1 * sigmoid(-y *( np.dot(w,x) + b))\n",
    "    \n",
    "    \"\"\"\n",
    "    #w,b  : fixed\n",
    "    for pn_sentence in batch_to_be_used:\n",
    "        y = pn_sentence[0]\n",
    "        #print(y)\n",
    "        x = pn_sentence[1]\n",
    "        x = np.array(x)\n",
    "        #print(x,len(w))\n",
    "        delL_devide_delw = np.array(delL_devide_delw) + (-1) * y * sigmoid((-1) * (y) * (np.dot(w,x)) )* x\n",
    "        delL_devide_delw_for_b += (-1) * y * sigmoid((-1) * (y) * (np.dot(w,x)) )* 1 # x内に成分1がappendされて,wxを考える\n",
    "        Loss += (-1) * math.log(ips +sigmoid(y * np.dot(w,x)))\n",
    "    #learning\n",
    "    w = w - ( learning_rate / len(batch_to_be_used) ) * delL_devide_delw\n",
    "    b  =  b -  ( learning_rate / len(batch_to_be_used) ) * delL_devide_delw_for_b\n",
    "    Loss = Loss / len(batch_to_be_used)\n",
    "    \n",
    "    return w,b,Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dev():\n",
    "    p_dev = []\n",
    "    n_dev = []\n",
    "    with open(\"./train_dev_test/p_dev\",\"rb\") as p:\n",
    "        with open(\"./train_dev_test/n_dev\",\"rb\") as n:\n",
    "            p_dev = pickle.load(p)\n",
    "            n_dev = pickle.load(n)\n",
    "    return p_dev, n_dev\n",
    "    \n",
    "def accuracy_eval(w,b):\n",
    "    p_dev,n_dev = load_dev()\n",
    "    correct = 0\n",
    "    all_count = 0\n",
    "    \n",
    "    for _ in p_dev:\n",
    "        all_count += 1\n",
    "        y_ = 1\n",
    "        _ = np.array(_)\n",
    "        if np.dot(w,_) + b >=0:\n",
    "            correct += 1\n",
    "    for _ in n_dev:\n",
    "        all_count += 1\n",
    "        y_ = -1\n",
    "        _ = np.array(_)\n",
    "        if ((np.dot(w,_) + b)) <0:\n",
    "            correct += 1    \n",
    "    accuracy = correct * 100 / all_count\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch1 end\n",
      "Loss: 3.386183156434444\n",
      "accuracy: 50.24875621890547\n",
      "epoch2 end\n",
      "Loss: 5.980204360482525\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "epoch_times =400\n",
    "\n",
    "load_batch = 100\n",
    "mini_batch_size = 10\n",
    "learning_rate = 1\n",
    "\n",
    "w_tr = np.random.rand(200000)\n",
    "b_tr = np.random.rand()\n",
    "\n",
    "Loss_list = []\n",
    "acc_list = []\n",
    "for _ in range(epoch_times):\n",
    "    p_trainbatch, n_trainbatch = load_train(load_batch)\n",
    "    w_, b_, Loss = training(w_tr,b_tr, p_trainbatch,n_trainbatch,learning_rate,mini_batch_size)\n",
    "    print(\"epoch\" + str(_ + 1) + \" end\")\n",
    "    print(\"Loss: \"+str(Loss))\n",
    "    accuracy = accuracy_eval( w_, b_ )\n",
    "    print(\"accuracy: \"+str(accuracy))\n",
    "    w_tr = w_\n",
    "    b_tr = b_\n",
    "    Loss_list.append(Loss)\n",
    "    acc_list.append(accuracy)\n",
    "    \n",
    "with open(\"./w_record_mb10_lr1\",\"wb\") as f:\n",
    "    pickle.dump(w_tr,f)\n",
    "    \n",
    "with open(\"./b_record_mb10_lr1\",\"wb\") as f:\n",
    "    pickle.dump(b_tr,f)\n",
    "    \n",
    "with open(\"./record/acc_record_mb10_lr1\",\"wb\") as f:\n",
    "    pickle.dump(acc_list,f)\n",
    "    \n",
    "with open(\"./record/loss_record_mb10_lr1\",\"wb\") as f:\n",
    "    pickle.dump(Loss_list,f)\n",
    "    \n",
    "plt.plot(Loss_list)\n",
    "plt.show()#Loss\n",
    "print()\n",
    "plt.figure()\n",
    "plt.plot(accuracy_list)\n",
    "plt.show()\n",
    "#accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "epoch_times =400\n",
    "\n",
    "load_batch = 100\n",
    "mini_batch_size = 50\n",
    "learning_rate = 1\n",
    "\n",
    "w_tr = np.random.rand(200000)\n",
    "b_tr = np.random.rand()\n",
    "\n",
    "Loss_list = []\n",
    "acc_list = []\n",
    "for _ in range(epoch_times):\n",
    "    p_trainbatch, n_trainbatch = load_train(load_batch)\n",
    "    w_, b_, Loss = training(w_tr,b_tr, p_trainbatch,n_trainbatch,learning_rate,mini_batch_size)\n",
    "    print(\"epoch\" + str(_ + 1) + \" end\")\n",
    "    print(\"Loss: \"+str(Loss))\n",
    "    accuracy = accuracy_eval( w_, b_ )\n",
    "    print(\"accuracy: \"+str(accuracy))\n",
    "    w_tr = w_\n",
    "    b_tr = b_\n",
    "    Loss_list.append(Loss)\n",
    "    acc_list.append(accuracy)\n",
    "    \n",
    "with open(\"./w_record_mb50_lr1\",\"wb\") as f:\n",
    "    pickle.dump(w_tr,f)\n",
    "    \n",
    "with open(\"./b_record_mb50_lr1\",\"wb\") as f:\n",
    "    pickle.dump(b_tr,f)\n",
    "    \n",
    "with open(\"./record/acc_record_mb50_lr1\",\"wb\") as f:\n",
    "    pickle.dump(acc_list,f)\n",
    "    \n",
    "with open(\"./record/loss_record_mb50_lr1\",\"wb\") as f:\n",
    "    pickle.dump(Loss_list,f)\n",
    "    \n",
    "plt.plot(Loss_list)\n",
    "plt.show()#Loss\n",
    "print()\n",
    "plt.figure()\n",
    "plt.plot(accuracy_list)\n",
    "plt.show()\n",
    "#accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

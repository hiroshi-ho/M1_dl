{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 sentence is vectorised\n",
      "500 sentence is vectorised\n",
      "0 sentence is vectorised\n",
      "500 sentence is vectorised\n",
      "2000 sentence is vectorised\n"
     ]
    }
   ],
   "source": [
    "#データをベクトル化する\n",
    "#かつ、データについてshuffleする。\n",
    "# -*- coding: utf-8 -*-\n",
    "import math\n",
    "import csv\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "shuffle_repeat = 100\n",
    "\n",
    "def sentence_vectorize():\n",
    "    \n",
    "    #positive,negativeすべて合わせた文のインデックス用\n",
    "    #後でシャッフル呼び出し時に用いる\n",
    "    total_index = 0\n",
    "    \n",
    "    #positive,negative両方用\n",
    "    all_sentence_vectorized = []\n",
    "    #positive : sign =1, negative : sign = -1\n",
    "    with open(\"./data/books/positive.review\",mode = \"r\", encoding = \"utf-8\") as p:\n",
    "        index = 0   #各文にインデックスを付ける。シャッフル呼び出し用\n",
    "        positive = []\n",
    "        for line in p:\n",
    "            \n",
    "            oneline = line.split(\" \") #まだ　\" \" でsplitしただけ\n",
    "            one_sentence = [0 for _ in range(200000)]  #これから一文をベクトル化する\n",
    "            \n",
    "            #一文内の各id:countに対する処理\n",
    "            for one_id_count_set in oneline:\n",
    "                id_,count_ = one_id_count_set.split(\":\")\n",
    "                # 文末のid:count用\n",
    "                count_ = count_.strip() \n",
    "                \n",
    "                one_sentence[int(id_)] += int(count_) \n",
    "\n",
    "            #一文に対する処理　ここまで\n",
    "            #one_sentenceを記録する、ただし、sign,indexをつける必要がある\n",
    "            \n",
    "            #sign, index用も込のリスト、すなわち\n",
    "            #[ sign ,  index,  [ 200000次元のリスト ]]　とする。\n",
    "            \n",
    "            \n",
    "            \n",
    "            if index % 500 == 0:\n",
    "                print(str(index) + \" sentence is vectorised\")\n",
    "            \n",
    "            positive.append(one_sentence)\n",
    "                \n",
    "            #各sentenceに固有\n",
    "            index +=1\n",
    "        # shuffle  \n",
    "        for i in range(shuffle_repeat):\n",
    "            positive = random.sample(positive,len(positive))\n",
    "        with open(\"./positive.review.vector\",\"wb\") as pp:\n",
    "            pickle.dump(positive,pp)\n",
    "        \n",
    "        #negative_dataの各文章に対するインデックス開始は1から\n",
    "        #total_indexは全センテンス用\n",
    "        total_index += index\n",
    "        all_sentence_vectorized.append(positive)\n",
    "\n",
    "    with open(\"./data/books/negative.review\",mode = \"r\", encoding = \"utf-8\") as n:\n",
    "        index = 0   #各文にインデックスを付ける。シャッフル呼び出し用\n",
    "        negative = []\n",
    "        for line in n:\n",
    "            \n",
    "            oneline = line.split(\" \") #まだ　\" \" でsplitしただけ\n",
    "            one_sentence = [0 for _ in range(200000)]  #これから一文をベクトル化する\n",
    "            \n",
    "            #一文内の各id:countに対する処理\n",
    "            for one_id_count_set in oneline:\n",
    "                id_,count_ = one_id_count_set.split(\":\")\n",
    "                # 文末のid:count用\n",
    "                count_ = count_.strip() \n",
    "                \n",
    "                one_sentence[int(id_)] += int(count_) \n",
    "            #一文に対する処理　ここまで\n",
    "            #one_sentenceを記録する、ただし、sign,indexをつける必要がある\n",
    "            \n",
    "            #sign, index用も込のリスト、すなわち\n",
    "            #[ sign ,  index,  [ 200000次元のリスト ]]　とする。\n",
    "            \n",
    "            #sentence_to_be_recorded = [index,-1,one_sentence]\n",
    "            \n",
    "            if index % 500 == 0:\n",
    "                print(str(index) + \" sentence is vectorised\")\n",
    "            \n",
    "            negative.append(one_sentence)\n",
    "                \n",
    "            #各sentenceに固有\n",
    "            index +=1\n",
    "            total_index +=1 \n",
    "        for i in range(shuffle_repeat):\n",
    "            negative = random.sample(negative,len(negative))\n",
    "    all_sentence_vectorized.append(negative)\n",
    "    \n",
    "   \n",
    "    with open(\"./negative.review.vector\",\"wb\") as nn:\n",
    "            pickle.dump(negative,nn) \n",
    "            \n",
    "        \n",
    "    with open(\"./all_sentence_vectorised.vector\",\"wb\") as f:\n",
    "        pickle.dump(all_sentence_vectorized,f)\n",
    "        \n",
    "    print(str(total_index) + \" sentence is vectorised\")\n",
    "    \n",
    "sentence_vectorize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split train=0.8, dev = 0.1, test = 0.1\n",
    "#bias is needed\n",
    "\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "def data_spliter():\n",
    "    p_train = []\n",
    "    p_dev = []\n",
    "    p_test = []\n",
    "    \n",
    "    n_train = []\n",
    "    n_dev = []\n",
    "    n_test = []\n",
    "    \n",
    "    with open(\"positive.review.vector\",\"rb\") as p:\n",
    "        p_list = pickle.load(p)\n",
    "\n",
    "        for i in range(len( p_list)):\n",
    "            if i <800:\n",
    "                p_train.append(p_list[i])\n",
    "            elif i >= 800 and i <= 900:\n",
    "                p_dev.append(p_list[i])\n",
    "            else:\n",
    "                p_test.append(p_list[i])\n",
    "                \n",
    "    with open(\"negative.review.vector\",\"rb\") as n:\n",
    "        n_list = pickle.load(n)\n",
    "\n",
    "        for i in range(len(n_list)):\n",
    "            if i <800:\n",
    "                n_train.append(n_list[i])\n",
    "            elif i >= 800 and i < 900:\n",
    "                n_dev.append(n_list[i])\n",
    "            else:\n",
    "                n_test.append(n_list[i])    \n",
    "        \n",
    "    with open(\"./train_dev_test/p_train\",\"wb\") as f:\n",
    "        pickle.dump(p_train,f)\n",
    "\n",
    "    with open(\"./train_dev_test/p_dev\",\"wb\") as f:\n",
    "        pickle.dump(p_dev,f)\n",
    "\n",
    "    with open(\"./train_dev_test/p_test\",\"wb\") as f:\n",
    "        pickle.dump(p_test,f)\n",
    "        \n",
    "    with open(\"./train_dev_test/n_train\",\"wb\") as f:\n",
    "        pickle.dump(n_train,f)\n",
    "\n",
    "    with open(\"./train_dev_test/n_dev\",\"wb\") as f:\n",
    "        pickle.dump(n_dev,f)\n",
    "\n",
    "    with open(\"./train_dev_test/n_test\",\"wb\") as f:\n",
    "        pickle.dump(n_test,f)\n",
    "        \n",
    "data_spliter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function\n",
    "\n",
    "import math\n",
    "def sigmoid(xx):\n",
    "    if xx > 0:\n",
    "        return (1 / (1 + math.exp(-xx)))\n",
    "    else:\n",
    "        return (1 - 1 / (1 + math.exp(xx)))\n",
    "        \n",
    "\n",
    "def v_dot(list_x,list_y): #内積\n",
    "    sums = 0\n",
    "    for i in range(len(list_x)):\n",
    "        sums+= list_x[i] * list_y[i]\n",
    "    return sums\n",
    "\n",
    "def v_plus(list_x,list_y):\n",
    "    result = [0 for _ in range(len(list_x))]\n",
    "    for i in range(len(list_x)):\n",
    "        result[i] = list_x[i] + list_y[i]\n",
    "    return result    \n",
    "\n",
    "def v_minus(list_x,list_y):\n",
    "    result = [0 for _ in range(len(list_x))]\n",
    "    for i in range(len(list_x)):\n",
    "        result[i] = list_x[i] - list_y[i]\n",
    "    return result    \n",
    "\n",
    "def v_a_fold(a,list_x): #vectorの定数倍\n",
    "    for i in list_x:\n",
    "        i = a * i\n",
    "    return list_x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([965, 521, 923, 370, 673, 279, 902, 644, 156, 946, 854, 209, 203,\n",
       "       207, 441, 157, 830, 321, 257, 330])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.choice([i for i in range(1000)],20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "def load_train(batch_size):\n",
    "    p_ret = []\n",
    "    n_ret = []\n",
    "    p_index = np.random.choice([i for i in range(800)],batch_size)\n",
    "    n_index = np.random.choice([i for i in range(800)],batch_size)\n",
    "    with open(\"./train_dev_test/p_train\",\"rb\") as p:\n",
    "        with open(\"./train_dev_test/n_train\",\"rb\") as n:\n",
    "            p_train = pickle.load(p)\n",
    "            n_train = pickle.load(n)\n",
    "            \n",
    "            for _ in p_index:\n",
    "                p_ret.append(p_train[_])\n",
    "                \n",
    "            for _ in n_index:\n",
    "                n_ret.append(n_train[_])\n",
    "    return p_ret, n_ret\n",
    "\n",
    "\n",
    "\n",
    "def training(w,b,p_batch,n_batch,learning_rate,mini_batch_size):\n",
    "    ips = 10 ** (-7)\n",
    "    \n",
    "    loss = 0\n",
    "    one_iter = np.array( [0 for i in range(200000)])\n",
    "    one_b = 0\n",
    "    \n",
    "    # 1 epochの中でp_batch(size = load_batch), n_batch(size = load_batch) 取ってきて\n",
    "    #　その中からランダムで　mini_batch　個のデータで　1epoch回す。\n",
    "    \n",
    "    \"\"\"\n",
    "    for batch in p_train_batchs :\n",
    "        y = 1\n",
    "        one_iter = v_plus(one_iter ,  v_a_fold(((-1) * (y) * sigmoid((-1) * y * np.dot(w,batch))),batch) )\n",
    "        one_b = one_b + (-1) * y * 1 * sigmoid((-1) * y * np.dot(w,batch))\n",
    "        loss += (-1) * math.log(ips + sigmoid(y * np.dot(w,one_iter) + b))    \n",
    "        #w = np.add(w ,(-1)* (learning_rate / len(batch)) * one_iter)\n",
    "        \n",
    "    w = np.add(w, v_a_fold((-1)* (learning_rate / len(p_train_batchs)) , one_iter) )\n",
    "    #b = np.add( b,  ((-1)* (learning_rate / len(batch)) )* one_b)\n",
    "    bias = bias - ((-1)* (learning_rate / len(p_train_batchs)) )* one_b      \n",
    "    print(\"Loss: \" + str(loss))\n",
    "    \n",
    "    loss = 0\n",
    "    one_iter = np.array( [0 for i in range(200000)])\n",
    "    one_b = 0\n",
    "    for batch in n_train_batchs :\n",
    "        y = -1\n",
    "        one_iter = v_plus(one_iter ,  v_a_fold(((-1) * (y) * sigmoid((-1) * y * np.dot(w,batch))),batch) )\n",
    "        one_b = one_b + (-1) * y * 1 * sigmoid((-1) * y * np.dot(w,batch))\n",
    "        loss += (-1) * math.log(ips + sigmoid(y * np.dot(w,one_iter) + b))    \n",
    "    #w = np.add(w ,(-1)* (learning_rate / len(batch)) * one_iter) \n",
    "    w = np.add(w, v_a_fold((-1)* (learning_rate / len(n_train_batchs)) , one_iter) )\n",
    "    #b = np.add( b,  ((-1)* (learning_rate / len(batch)) )* one_b)\n",
    "    bias = bias - ((-1)* (learning_rate / len(n_train_batchs)) )* one_b      \n",
    "    print(\"Loss: \" + str(loss))    \n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    #print(\"p_batch\"+str(p_batch))\n",
    "\n",
    "    indexed_sentence = []\n",
    "    for sentence in p_batch:\n",
    "        indexed_sentence.append([1,sentence])\n",
    "        \n",
    "    for sentence in n_batch:\n",
    "        indexed_sentence.append([-1,sentence])\n",
    "        \n",
    "    batch_to_be_used_index = np.random.choice(len(indexed_sentence),mini_batch_size)\n",
    "    batch_to_be_used = []\n",
    "    for _ in batch_to_be_used_index:\n",
    "        batch_to_be_used.append(indexed_sentence[_])\n",
    "    \n",
    "    #print(batch_to_be_used)\n",
    "    Loss = 0\n",
    "    delL_devide_delw = np.array([0 for _ in range(200000)])\n",
    "    delL_devide_delw_for_b = 0\n",
    "    \n",
    "    \"\"\"\n",
    "    delL_devide_delw = -y * sigmoid(-y *( np.dot(w,x) + b)) * x\n",
    "    delL_devide_delw_for_b  = (-y) * 1 * sigmoid(-y *( np.dot(w,x) + b))\n",
    "    \n",
    "    \"\"\"\n",
    "    #w,b  : fixed\n",
    "    for pn_sentence in batch_to_be_used:\n",
    "        y = pn_sentence[0]\n",
    "        #print(y)\n",
    "        x = pn_sentence[1]\n",
    "        x = np.array(x)\n",
    "        #print(x,len(w))\n",
    "        delL_devide_delw = np.array(delL_devide_delw) + (-1) * y * sigmoid((-1) * (y) * (np.dot(w,x)) )* x\n",
    "        delL_devide_delw_for_b += (-1) * y * sigmoid((-1) * (y) * (np.dot(w,x)) )* 1 # x内に成分1がappendされて,wxを考える\n",
    "        Loss += (-1) * math.log(ips +sigmoid(y * np.dot(w,x)))\n",
    "    #learning\n",
    "    w = w - ( learning_rate / len(batch_to_be_used) ) * delL_devide_delw\n",
    "    b  =  b -  ( learning_rate / len(batch_to_be_used) ) * delL_devide_delw_for_b\n",
    "    Loss = Loss / len(batch_to_be_used)\n",
    "    \n",
    "    return w,b,Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dev():\n",
    "    p_dev = []\n",
    "    n_dev = []\n",
    "    with open(\"./train_dev_test/p_dev\",\"rb\") as p:\n",
    "        with open(\"./train_dev_test/n_dev\",\"rb\") as n:\n",
    "            p_dev = pickle.load(p)\n",
    "            n_dev = pickle.load(n)\n",
    "    return p_dev, n_dev\n",
    "    \n",
    "def accuracy_eval(w,b):\n",
    "    p_dev,n_dev = load_dev()\n",
    "    correct = 0\n",
    "    all_count = 0\n",
    "    \n",
    "    for _ in p_dev:\n",
    "        all_count += 1\n",
    "        y_ = 1\n",
    "        _ = np.array(_)\n",
    "        if np.dot(w,_) + b >=0:\n",
    "            correct += 1\n",
    "    for _ in n_dev:\n",
    "        all_count += 1\n",
    "        y_ = -1\n",
    "        _ = np.array(_)\n",
    "        if ((np.dot(w,_) + b)) <0:\n",
    "            correct += 1    \n",
    "    accuracy = correct * 100 / all_count\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch1 end\n",
      "Loss: 2.4285127753263724\n",
      "accuracy: 50.24875621890547\n",
      "epoch2 end\n",
      "Loss: 4.413280672131117\n",
      "accuracy: 50.24875621890547\n",
      "epoch3 end\n",
      "Loss: 2.708014429409851\n",
      "accuracy: 50.24875621890547\n",
      "epoch4 end\n",
      "Loss: 3.3577462761204537\n",
      "accuracy: 50.24875621890547\n",
      "epoch5 end\n",
      "Loss: 3.334225065121013\n",
      "accuracy: 48.756218905472636\n",
      "epoch6 end\n",
      "Loss: 3.2339428428136285\n",
      "accuracy: 48.756218905472636\n",
      "epoch7 end\n",
      "Loss: 1.5279791972772587\n",
      "accuracy: 48.25870646766169\n",
      "epoch8 end\n",
      "Loss: 2.1804298172417313\n",
      "accuracy: 49.25373134328358\n",
      "epoch9 end\n",
      "Loss: 11.661960074608075\n",
      "accuracy: 46.766169154228855\n",
      "epoch10 end\n",
      "Loss: 2.1339382675378973\n",
      "accuracy: 46.26865671641791\n",
      "epoch11 end\n",
      "Loss: 6.206453159884203\n",
      "accuracy: 46.26865671641791\n",
      "epoch12 end\n",
      "Loss: 3.330428795222339\n",
      "accuracy: 45.27363184079602\n",
      "epoch13 end\n",
      "Loss: 11.724050408822148\n",
      "accuracy: 47.76119402985075\n",
      "epoch14 end\n",
      "Loss: 5.420869948143198\n",
      "accuracy: 51.243781094527364\n",
      "epoch15 end\n",
      "Loss: 3.444220755381393\n",
      "accuracy: 50.74626865671642\n",
      "epoch16 end\n",
      "Loss: 4.620538911303326\n",
      "accuracy: 52.23880597014925\n",
      "epoch17 end\n",
      "Loss: 4.2465106691452315\n",
      "accuracy: 52.23880597014925\n",
      "epoch18 end\n",
      "Loss: 4.722623759926281\n",
      "accuracy: 52.23880597014925\n",
      "epoch19 end\n",
      "Loss: 3.5103555315721033\n",
      "accuracy: 52.7363184079602\n",
      "epoch20 end\n",
      "Loss: 3.3583486094757973\n",
      "accuracy: 54.72636815920398\n",
      "epoch21 end\n",
      "Loss: 2.6199497828393845\n",
      "accuracy: 53.233830845771145\n",
      "epoch22 end\n",
      "Loss: 7.258954394946564\n",
      "accuracy: 53.233830845771145\n",
      "epoch23 end\n",
      "Loss: 0.28895614467420444\n",
      "accuracy: 53.73134328358209\n",
      "epoch24 end\n",
      "Loss: 3.745883100657821\n",
      "accuracy: 54.22885572139303\n",
      "epoch25 end\n",
      "Loss: 2.2049279601173497\n",
      "accuracy: 53.233830845771145\n",
      "epoch26 end\n",
      "Loss: 1.2892750155500905\n",
      "accuracy: 52.7363184079602\n",
      "epoch27 end\n",
      "Loss: 2.2189082208915445\n",
      "accuracy: 52.7363184079602\n",
      "epoch28 end\n",
      "Loss: 4.751535845579895\n",
      "accuracy: 52.23880597014925\n",
      "epoch29 end\n",
      "Loss: 8.383397573603258\n",
      "accuracy: 51.243781094527364\n",
      "epoch30 end\n",
      "Loss: 4.34060033174733\n",
      "accuracy: 50.24875621890547\n",
      "epoch31 end\n",
      "Loss: 4.648123151855445\n",
      "accuracy: 52.23880597014925\n",
      "epoch32 end\n",
      "Loss: 2.5659372461301797\n",
      "accuracy: 51.74129353233831\n",
      "epoch33 end\n",
      "Loss: 4.320298500159792\n",
      "accuracy: 51.74129353233831\n",
      "epoch34 end\n",
      "Loss: 1.5659428121196917\n",
      "accuracy: 52.23880597014925\n",
      "epoch35 end\n",
      "Loss: 2.116339836548724\n",
      "accuracy: 51.74129353233831\n",
      "epoch36 end\n",
      "Loss: 3.638151208011115\n",
      "accuracy: 53.233830845771145\n",
      "epoch37 end\n",
      "Loss: 0.6240322804395966\n",
      "accuracy: 53.233830845771145\n",
      "epoch38 end\n",
      "Loss: 3.019326360702405\n",
      "accuracy: 53.233830845771145\n",
      "epoch39 end\n",
      "Loss: 1.1621724823726922\n",
      "accuracy: 53.233830845771145\n",
      "epoch40 end\n",
      "Loss: 3.813947445076212\n",
      "accuracy: 54.22885572139303\n",
      "epoch41 end\n",
      "Loss: 0.7361662817539998\n",
      "accuracy: 53.73134328358209\n",
      "epoch42 end\n",
      "Loss: 1.3259300408053685\n",
      "accuracy: 53.73134328358209\n",
      "epoch43 end\n",
      "Loss: 4.03331436625639\n",
      "accuracy: 53.73134328358209\n",
      "epoch44 end\n",
      "Loss: 1.5124102869212357\n",
      "accuracy: 53.73134328358209\n",
      "epoch45 end\n",
      "Loss: 2.0659792530016925\n",
      "accuracy: 52.7363184079602\n",
      "epoch46 end\n",
      "Loss: 1.8574821586627526\n",
      "accuracy: 52.7363184079602\n",
      "epoch47 end\n",
      "Loss: 2.2664263414749266\n",
      "accuracy: 52.7363184079602\n",
      "epoch48 end\n",
      "Loss: 4.27997835559084\n",
      "accuracy: 52.7363184079602\n",
      "epoch49 end\n",
      "Loss: 0.5198200348596442\n",
      "accuracy: 52.7363184079602\n",
      "epoch50 end\n",
      "Loss: 2.0957316185213957\n",
      "accuracy: 52.7363184079602\n",
      "epoch51 end\n",
      "Loss: 0.9167985799604974\n",
      "accuracy: 52.7363184079602\n",
      "epoch52 end\n",
      "Loss: 4.234437779884198\n",
      "accuracy: 52.7363184079602\n",
      "epoch53 end\n",
      "Loss: 2.1688292459166156\n",
      "accuracy: 52.7363184079602\n",
      "epoch54 end\n",
      "Loss: 3.9263985438572484\n",
      "accuracy: 52.7363184079602\n",
      "epoch55 end\n",
      "Loss: 1.374023331251829\n",
      "accuracy: 52.23880597014925\n",
      "epoch56 end\n",
      "Loss: 2.7608682701501612\n",
      "accuracy: 52.23880597014925\n",
      "epoch57 end\n",
      "Loss: 4.84231435816189\n",
      "accuracy: 52.23880597014925\n",
      "epoch58 end\n",
      "Loss: 3.857000237958906\n",
      "accuracy: 52.7363184079602\n",
      "epoch59 end\n",
      "Loss: 4.9921661153235615\n",
      "accuracy: 52.23880597014925\n",
      "epoch60 end\n",
      "Loss: 2.5831889374807515\n",
      "accuracy: 52.23880597014925\n",
      "epoch61 end\n",
      "Loss: 0.9063694512950985\n",
      "accuracy: 52.23880597014925\n",
      "epoch62 end\n",
      "Loss: 1.0420359971058444\n",
      "accuracy: 52.23880597014925\n",
      "epoch63 end\n",
      "Loss: 7.327418567485855\n",
      "accuracy: 52.23880597014925\n",
      "epoch64 end\n",
      "Loss: 0.6668808682371574\n",
      "accuracy: 52.23880597014925\n",
      "epoch65 end\n",
      "Loss: 4.6020726546842425\n",
      "accuracy: 52.23880597014925\n",
      "epoch66 end\n",
      "Loss: 1.5351820524138822\n",
      "accuracy: 52.23880597014925\n",
      "epoch67 end\n",
      "Loss: 2.516878305670386\n",
      "accuracy: 52.23880597014925\n",
      "epoch68 end\n",
      "Loss: 0.5867994736174537\n",
      "accuracy: 52.23880597014925\n",
      "epoch69 end\n",
      "Loss: 1.120504795278225\n",
      "accuracy: 52.23880597014925\n",
      "epoch70 end\n",
      "Loss: 0.9394125042652911\n",
      "accuracy: 52.23880597014925\n",
      "epoch71 end\n",
      "Loss: 1.0934337673168806\n",
      "accuracy: 52.23880597014925\n",
      "epoch72 end\n",
      "Loss: 3.656509161797453\n",
      "accuracy: 51.74129353233831\n",
      "epoch73 end\n",
      "Loss: 1.0277369398630087\n",
      "accuracy: 51.74129353233831\n",
      "epoch74 end\n",
      "Loss: 2.5747605941905727\n",
      "accuracy: 51.74129353233831\n",
      "epoch75 end\n",
      "Loss: 1.15372504208328\n",
      "accuracy: 51.74129353233831\n",
      "epoch76 end\n",
      "Loss: 2.5043571437363967\n",
      "accuracy: 51.74129353233831\n",
      "epoch77 end\n",
      "Loss: 1.8472756625216593\n",
      "accuracy: 51.74129353233831\n",
      "epoch78 end\n",
      "Loss: 2.2814368091927806\n",
      "accuracy: 51.74129353233831\n",
      "epoch79 end\n",
      "Loss: 0.886706705067066\n",
      "accuracy: 51.74129353233831\n",
      "epoch80 end\n",
      "Loss: 0.08691377285594809\n",
      "accuracy: 51.243781094527364\n",
      "epoch81 end\n",
      "Loss: 5.621878069493492\n",
      "accuracy: 50.24875621890547\n",
      "epoch82 end\n",
      "Loss: 0.403914728916188\n",
      "accuracy: 50.24875621890547\n",
      "epoch83 end\n",
      "Loss: 0.25490319637673287\n",
      "accuracy: 50.24875621890547\n",
      "epoch84 end\n",
      "Loss: 1.1697283578711843\n",
      "accuracy: 50.24875621890547\n",
      "epoch85 end\n",
      "Loss: 7.065931224520826\n",
      "accuracy: 50.24875621890547\n",
      "epoch86 end\n",
      "Loss: 2.254222725097713\n",
      "accuracy: 50.24875621890547\n",
      "epoch87 end\n",
      "Loss: 0.982448866011741\n",
      "accuracy: 50.24875621890547\n",
      "epoch88 end\n",
      "Loss: 0.6978983024095277\n",
      "accuracy: 50.24875621890547\n",
      "epoch89 end\n",
      "Loss: 1.4627100883128175\n",
      "accuracy: 50.24875621890547\n",
      "epoch90 end\n",
      "Loss: 4.020635658749426\n",
      "accuracy: 50.24875621890547\n",
      "epoch91 end\n",
      "Loss: 0.8894702538933249\n",
      "accuracy: 50.24875621890547\n",
      "epoch92 end\n",
      "Loss: 1.027729428783702\n",
      "accuracy: 50.24875621890547\n",
      "epoch93 end\n",
      "Loss: 2.32949231644611\n",
      "accuracy: 50.24875621890547\n",
      "epoch94 end\n",
      "Loss: 1.5680932218245416\n",
      "accuracy: 50.24875621890547\n",
      "epoch95 end\n",
      "Loss: 0.6948338154685685\n",
      "accuracy: 50.24875621890547\n",
      "epoch96 end\n",
      "Loss: 2.013472755174349\n",
      "accuracy: 50.24875621890547\n",
      "epoch97 end\n",
      "Loss: 1.8559830405983875\n",
      "accuracy: 50.24875621890547\n",
      "epoch98 end\n",
      "Loss: 0.8118940717559857\n",
      "accuracy: 50.24875621890547\n",
      "epoch99 end\n",
      "Loss: 1.095639612132366\n",
      "accuracy: 50.24875621890547\n",
      "epoch100 end\n",
      "Loss: 1.7168091041626656\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "epoch_times =400\n",
    "\n",
    "load_batch = 100\n",
    "mini_batch_size = 5\n",
    "learning_rate = 1\n",
    "\n",
    "w_tr = np.random.rand(200000)\n",
    "b_tr = np.random.rand()\n",
    "\n",
    "Loss_list = []\n",
    "acc_list = []\n",
    "for _ in range(epoch_times):\n",
    "    p_trainbatch, n_trainbatch = load_train(load_batch)\n",
    "    w_, b_, Loss = training(w_tr,b_tr, p_trainbatch,n_trainbatch,learning_rate,mini_batch_size)\n",
    "    print(\"epoch\" + str(_ + 1) + \" end\")\n",
    "    print(\"Loss: \"+str(Loss))\n",
    "    accuracy = accuracy_eval( w_, b_ )\n",
    "    print(\"accuracy: \"+str(accuracy))\n",
    "    w_tr = w_\n",
    "    b_tr = b_\n",
    "    Loss_list.append(Loss)\n",
    "    acc_list.append(accuracy)\n",
    "    \n",
    "with open(\"./w_record_mb5_lr1\",\"wb\") as f:\n",
    "    pickle.dump(w_tr,f)\n",
    "    \n",
    "with open(\"./b_record_mb5_lr1\",\"wb\") as f:\n",
    "    pickle.dump(b_tr,f)\n",
    "    \n",
    "with open(\"./record/acc_record_mb5_lr1\",\"wb\") as f:\n",
    "    pickle.dump(acc_list,f)\n",
    "    \n",
    "with open(\"./record/loss_record_mb5_lr1\",\"wb\") as f:\n",
    "    pickle.dump(Loss_list,f)\n",
    "    \n",
    "plt.plot(Loss_list)\n",
    "plt.show()#Loss\n",
    "print()\n",
    "plt.figure()\n",
    "plt.plot(accuracy_list)\n",
    "plt.show()\n",
    "#accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "epoch_times =400\n",
    "\n",
    "load_batch = 100\n",
    "mini_batch_size = 5\n",
    "learning_rate = 0.1\n",
    "\n",
    "w_tr = np.random.rand(200000)\n",
    "b_tr = np.random.rand()\n",
    "\n",
    "Loss_list = []\n",
    "acc_list = []\n",
    "for _ in range(epoch_times):\n",
    "    p_trainbatch, n_trainbatch = load_train(load_batch)\n",
    "    w_, b_, Loss = training(w_tr,b_tr, p_trainbatch,n_trainbatch,learning_rate,mini_batch_size)\n",
    "    print(\"epoch\" + str(_ + 1) + \" end\")\n",
    "    print(\"Loss: \"+str(Loss))\n",
    "    accuracy = accuracy_eval( w_, b_ )\n",
    "    print(\"accuracy: \"+str(accuracy))\n",
    "    w_tr = w_\n",
    "    b_tr = b_\n",
    "    Loss_list.append(Loss)\n",
    "    acc_list.append(accuracy)\n",
    "    \n",
    "with open(\"./w_record_mb5_lr0p1\",\"wb\") as f:\n",
    "    pickle.dump(w_tr,f)\n",
    "    \n",
    "with open(\"./b_record_mb5_lr0p1\",\"wb\") as f:\n",
    "    pickle.dump(b_tr,f)\n",
    "    \n",
    "with open(\"./record/acc_record_mb5_lr0p1\",\"wb\") as f:\n",
    "    pickle.dump(acc_list,f)\n",
    "    \n",
    "with open(\"./record/loss_record_mb5_lr0p1\",\"wb\") as f:\n",
    "    pickle.dump(Loss_list,f)\n",
    "    \n",
    "plt.plot(Loss_list)\n",
    "plt.show()#Loss\n",
    "print()\n",
    "plt.figure()\n",
    "plt.plot(accuracy_list)\n",
    "plt.show()\n",
    "#accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
